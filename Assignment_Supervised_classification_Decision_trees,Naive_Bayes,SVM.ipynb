{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question 1 :  What is Information Gain, and how is it used in Decision Trees?\n",
        "\n",
        "Answer:-Information Gain tells us which feature best separates the data into distinct classes.Information Gain (IG) is a concept from information theory that measures how much uncertainty (impurity) in a dataset is reduced after splitting the data based on a particular feature.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "*   Information Gain measures reduction in entropy\n",
        "\n",
        "*   Used to choose the best split in Decision Trees\n",
        "\n",
        "**How Information Gain is used in Decision Trees**  \n",
        "\n",
        "\n",
        "* Measures Reduction in Entropy:\n",
        "Information Gain quantifies how much the uncertainty (or impurity) in a dataset is reduced after a split. A higher Information Gain means a better split.\n",
        "\n",
        "\n",
        "\n",
        "*  Feature Selection:When building a Decision Tree, at each step, the algorithm evaluates all available features to see which one provides the most significant Information Gain if used to split the data. The feature that yields the highest Information Gain is chosen for that particular split.\n",
        "*   Goal: The ultimate goal is to create homogeneous child nodes, meaning each child node contains data points predominantly belonging to a single class. Information Gain helps in achieving this homogeneity efficiently by selecting the most informative features.\n",
        "\n"
      ],
      "metadata": {
        "id": "6aKGfUKkJg_s"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "028515e9"
      },
      "source": [
        "Question 2: What is the difference between Gini Impurity and Entropy?\n",
        "\n",
        "Both Gini Impurity and Entropy are measures of impurity or disorder used in the construction of decision trees to determine the optimal split at each node. The goal is to minimize impurity after a split.\n",
        "\n",
        "Entropy\n",
        "\n",
        "*   **Definition:** Entropy is a concept from information theory that measures the average level of uncertainty or surprise inherent in a variable's possible outcomes. In the context of decision trees, it quantifies the impurity of a dataset.\n",
        "*   **Formula:** $Entropy(S) = -\\sum_{i=1}^{c} p_i \\log_2(p_i)$, where $S$ is the dataset, $c$ is the number of classes, and $p_i$ is the proportion of observations belonging to class $i$.\n",
        "*   **Interpretation:**\n",
        "    *   An Entropy of 0 means the node is perfectly pure (all samples belong to the same class).\n",
        "    *   An Entropy of 1 (for a binary classification) means the node is perfectly impure (samples are equally divided among classes).\n",
        "*   **Strengths:**\n",
        "    *   More sensitive to changes in class probabilities, meaning it might lead to slightly more balanced trees.\n",
        "    *   Often preferred when a more robust measure of uncertainty is desired.\n",
        "*   **Weaknesses:**\n",
        "    *   Involves logarithmic calculations, which can be computationally more expensive than Gini Impurity.\n",
        "*   **Use Cases:** Often used in algorithms like ID3 and C4.\n",
        "\n",
        "**Gini Impurity**\n",
        "\n",
        "*   **Definition:** Gini Impurity measures the probability of incorrectly classifying a randomly chosen element in the dataset if that element were randomly labeled according to the distribution of labels in the dataset.\n",
        "*   **Formula:** $Gini(S) = 1 - \\sum_{i=1}^{c} p_i^2$, where $S$ is the dataset, $c$ is the number of classes, and $p_i$ is the proportion of observations belonging to class $i$.\n",
        "*   **Interpretation:**\n",
        "    *   A Gini Impurity of 0 means the node is perfectly pure (all samples belong to the same class).\n",
        "    *   A Gini Impurity of 0.5 (for a binary classification) means the node is perfectly impure (samples are equally divided among classes).\n",
        "*   **Strengths:**\n",
        "    *   Computationally less intensive as it avoids logarithmic calculations.\n",
        "    *   Favors larger partitions, which can sometimes lead to slight bias towards the majority class in a split.\n",
        "*   **Weaknesses:**\n",
        "    *   Can be less sensitive to class probability changes compared to Entropy.\n",
        "*   **Use Cases:** Widely used in algorithms like CART (Classification and Regression Trees).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ca951c30"
      },
      "source": [
        "Question 3: What is Pre-Pruning in Decision Trees?\n",
        "\n",
        "Pre-pruning, also known as early stopping, is a technique used in the construction of decision trees to prevent overfitting. Instead of growing a full decision tree and then pruning it back (post-pruning), pre-pruning involves stopping the tree growth prematurely if certain conditions are met.\n",
        "\n",
        "The main idea behind pre-pruning is to restrict the complexity of the decision tree during its formation, thus reducing the risk of the tree learning noise in the training data and improving its generalization ability on unseen data.\n",
        "\n",
        "### How Pre-Pruning Works:\n",
        "\n",
        "During the tree building process, at each node, before a split is performed, a pre-pruning criterion is evaluated. If the criterion is met, the tree growth is halted at that node, and the node is made a leaf node. The class label for this leaf node is typically determined by the majority class of the samples reaching that node.\n",
        "\n",
        "### Common Pre-Pruning Criteria:\n",
        "\n",
        "Several criteria can be used to decide when to stop splitting a node:\n",
        "\n",
        "1.  **Maximum Depth:** The tree stops growing once it reaches a predefined maximum depth. This limits the number of sequential splits that can occur.\n",
        "2.  **Minimum Samples per Split:** A node will not be split if the number of samples in that node is below a certain threshold. This prevents creating splits on very small subsets of data, which might be noisy.\n",
        "3.  **Minimum Samples per Leaf Node:** A split will only be considered if it results in child nodes that each contain at least a specified minimum number of samples. This ensures that leaf nodes are not too small.\n",
        "4.  **Maximum Number of Leaf Nodes:** The tree growth stops when the total number of leaf nodes reaches a predefined limit.\n",
        "5.  **Impurity Threshold:** A node will not be split if its impurity (e.g., Gini impurity or entropy) is below a certain threshold. If a node is already 'pure enough,' further splitting might not provide significant gain.\n",
        "6.  **Information Gain/Impurity Decrease Threshold:** A split is only performed if the information gain (or decrease in impurity) achieved by that split is above a certain minimum value. If the gain is too small, the split is considered insignificant and is not made.\n",
        "7.  **Cross-Validation Score:** The tree's performance on a validation set (or through cross-validation) can be monitored. If splitting a node leads to a decrease in validation performance, the split is rejected.\n",
        "\n",
        "### Advantages of Pre-Pruning:\n",
        "\n",
        "*   **Reduces Overfitting:** Directly addresses overfitting by limiting tree complexity.\n",
        "*   **Faster Training:** Since the tree stops growing earlier, the training process is generally faster than building a full tree and then post-pruning it.\n",
        "*   **Simpler Trees:** Tends to produce smaller and more interpretable trees.\n",
        "*   **Computational Efficiency:** Avoids the computational cost of generating unnecessary branches.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "068c497e"
      },
      "source": [
        "Question 4: Train a Decision Tree Classifier using Gini Impurity and print feature importances.\n",
        "\n",
        "This program will demonstrate how to train a Decision Tree Classifier using `criterion='gini'` and then retrieve and display the feature importances. We'll use the well-known Iris dataset for this example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9097d22e",
        "outputId": "1ca5f0f3-922e-44ed-bdeb-b0556cf2e093"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 1. Load a sample dataset (Iris dataset)\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "feature_names = iris.feature_names\n",
        "\n",
        "print(f\"Dataset features: {feature_names}\")\n",
        "print(f\"Shape of data (X): {X.shape}\")\n",
        "print(f\"Shape of target (y): {y.shape}\\n\")\n",
        "\n",
        "# 2. Split data into training and testing sets (optional but good practice)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# 3. Initialize the Decision Tree Classifier with Gini Impurity\n",
        "#    criterion='gini' is the default, but we explicitly set it for clarity.\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "\n",
        "# 4. Train the classifier on the training data\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "print(\"Decision Tree Classifier trained successfully using Gini Impurity.\\n\")\n",
        "\n",
        "# 5. Print the feature importances\n",
        "print(\"Feature Importances:\")\n",
        "for i, importance in enumerate(clf.feature_importances_):\n",
        "    print(f\"  {feature_names[i]}: {importance:.4f}\")\n",
        "\n",
        "# You can also get predictions and evaluate the model if needed\n",
        "# from sklearn.metrics import accuracy_score\n",
        "# y_pred = clf.predict(X_test)\n",
        "# accuracy = accuracy_score(y_test, y_pred)\n",
        "# print(f\"\\nModel Accuracy on Test Set: {accuracy:.4f}\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset features: ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
            "Shape of data (X): (150, 4)\n",
            "Shape of target (y): (150,)\n",
            "\n",
            "Decision Tree Classifier trained successfully using Gini Impurity.\n",
            "\n",
            "Feature Importances:\n",
            "  sepal length (cm): 0.0000\n",
            "  sepal width (cm): 0.0191\n",
            "  petal length (cm): 0.8933\n",
            "  petal width (cm): 0.0876\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adf918ed"
      },
      "source": [
        "Question 5: What is a Support Vector Machine (SVM)?\n",
        "\n",
        "A **Support Vector Machine (SVM)** is a powerful and versatile machine learning algorithm capable of performing linear or non-linear classification, regression, and even outlier detection. It is primarily used for classification tasks.\n",
        "\n",
        "### Core Idea:\n",
        "\n",
        "The fundamental concept behind SVMs is to find the \"best\" hyperplane that separates data points belonging to different classes in a high-dimensional space. The \"best\" hyperplane is defined as the one with the largest margin between the two classes.\n",
        "\n",
        "*   **Hyperplane:** In a 2D space, a hyperplane is a line. In a 3D space, it's a plane. In spaces with more than three dimensions, it's called a hyperplane.\n",
        "*   **Margin:** The margin is the distance between the hyperplane and the closest data points from each class. These closest points are called **support vectors**.\n",
        "\n",
        "### How SVM Works (for Classification):\n",
        "\n",
        "1.  **Finding the Optimal Hyperplane:** The SVM algorithm aims to find a hyperplane that maximizes the margin. A larger margin generally means lower generalization error of the classifier.\n",
        "\n",
        "2.  **Support Vectors:** The data points that lie closest to the decision boundary (hyperplane) and influence its position and orientation are called support vectors. These are the critical elements of the training set; if you remove them, the hyperplane's position would change.\n",
        "\n",
        "3.  **Handling Non-linear Data (Kernel Trick):** SVMs are incredibly effective because they can handle non-linearly separable data through a technique called the **kernel trick**. When data points cannot be separated by a straight line (or plane) in their original dimension, the kernel trick implicitly maps the input data into a higher-dimensional feature space where it might become linearly separable. Common kernel functions include:\n",
        "    *   **Linear Kernel:** Used for linearly separable data.\n",
        "    *   **Polynomial Kernel:** Projects data into a higher dimension using polynomial functions.\n",
        "    *   **Radial Basis Function (RBF) or Gaussian Kernel:** A popular choice for complex, non-linear relationships.\n",
        "\n",
        "4.  **Soft Margin Classification:** In real-world scenarios, data is often noisy, and perfect linear separation might not be possible, or it might lead to overfitting. SVMs can handle this using **soft margin classification**. This allows some instances to be on the wrong side of the margin, or even the wrong side of the hyperplane, by introducing a hyperparameter `C`. A smaller `C` allows more margin violations (more regularization), while a larger `C` tries to keep all instances off the margin or on the correct side (less regularization).\n",
        "\n",
        "### Key Features and Advantages:\n",
        "\n",
        "*   **Effective in High-Dimensional Spaces:** Works well even when the number of features is greater than the number of samples.\n",
        "*   **Memory Efficient:** Uses a subset of training points (support vectors) in the decision function, making it memory efficient.\n",
        "*   **Versatile:** Can be used for both linear and non-linear classification and regression problems with different kernel functions.\n",
        "*   **Robust to Outliers:** With soft margin classification, SVMs can be less sensitive to individual noisy data points.\n",
        "\n",
        "### Disadvantages:\n",
        "\n",
        "*   **Computationally Intensive:** Can be computationally expensive for very large datasets, especially with complex kernels.\n",
        "*   **Sensitive to Feature Scaling:** SVMs are sensitive to the scaling of features. It's often recommended to normalize or standardize data before applying SVM.\n",
        "*   **Kernel Choice:** Choosing the right kernel and its parameters can be challenging and often requires domain knowledge or extensive hyperparameter tuning.\n",
        "*   **Lack of Probability Estimates:** Standard SVMs do not directly provide probability estimates. They output raw decision values, which can be converted to probabilities using techniques like Platt scaling, but this adds another layer of computation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e109e52f"
      },
      "source": [
        "Question 6: What is the Kernel Trick in SVM?\n",
        "\n",
        "The **Kernel Trick** is a fundamental concept that allows Support Vector Machines (SVMs) to effectively handle non-linearly separable data. In its essence, it's a method of using a kernel function to implicitly map input data into a higher-dimensional feature space, where it might become linearly separable, without actually calculating the coordinates of the data in that higher-dimensional space.\n",
        "\n",
        " Why is it needed?\n",
        "\n",
        "Many real-world datasets are not linearly separable in their original feature space. This means you cannot draw a single straight line (or hyperplane in higher dimensions) to perfectly separate the different classes. For example, if you have data points forming concentric circles, a linear classifier would fail to distinguish between the inner and outer circles.\n",
        "\n",
        "How it Works:\n",
        "\n",
        "Instead of explicitly transforming the data into a higher dimension (which can be computationally expensive and memory intensive, especially for very high dimensions), the kernel trick uses a **kernel function** `K(x, x')` that calculates the dot product of the transformed features, `φ(x) · φ(x')`, directly in the original feature space. That is:\n",
        "\n",
        "$K(x, x') = φ(x) · φ(x')$\n",
        "\n",
        "Where:\n",
        "*   `x` and `x'` are data points in the original feature space.\n",
        "*   `φ` is the mapping function that transforms the data into a higher-dimensional space.\n",
        "*   `K` is the kernel function.\n",
        "\n",
        "This means that the SVM algorithm, which relies on dot products to calculate distances and angles between data points, can operate in the implicitly higher-dimensional space without ever needing to know the `φ` function or the explicit coordinates of the transformed data points.\n",
        "\n",
        "Benefits of the Kernel Trick:\n",
        "\n",
        "*   **Handles Non-Linear Separability:** The primary benefit is the ability to classify data that is not linearly separable in its original feature space.\n",
        "*   **Computational Efficiency:** Avoids the explicit computation of coordinates in high-dimensional feature spaces, which can be computationally prohibitive. The kernel function itself is often much simpler and faster to compute.\n",
        "*   **Memory Efficiency:** No need to store the high-dimensional transformed data points.\n",
        "*   **Flexibility:** Different kernel functions can be chosen to suit various types of non-linear relationships in the data.\n",
        "\n",
        "Common Kernel Functions:\n",
        "\n",
        "1.  **Linear Kernel:**\n",
        "    $K(x, x') = x · x'$\n",
        "    (Equivalent to no transformation, used for linearly separable data)\n",
        "\n",
        "2.  **Polynomial Kernel:**\n",
        "    $K(x, x') = (γx · x' + r)^d$\n",
        "\n",
        "    (Maps data to a higher-dimensional space using polynomial functions, with `d` as the degree of the polynomial, `γ` as a scaling factor, and `r` as a constant term.)\n",
        "\n",
        "3.  **Radial Basis Function (RBF) / Gaussian Kernel:**\n",
        "    $K(x, x') = e^{(-γ||x - x'||^2)}$\n",
        "\n",
        "    (A very popular choice for complex, non-linear relationships. It creates a spherical decision boundary. `γ` is a parameter that defines the influence of a single training example).\n",
        "\n",
        "4.  **Sigmoid Kernel:**\n",
        "    $K(x, x') = \\tanh(γx · x' + r)$\n",
        "    \n",
        "    (Often used in neural networks, but less common for SVMs compared to RBF or polynomial).\n",
        "\n",
        "In summary, the kernel trick is a powerful mathematical tool that allows SVMs to find non-linear decision boundaries by implicitly operating in a higher-dimensional space, making them highly effective for a wide range of complex classification problems."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b11e99d7"
      },
      "source": [
        "Question 7: Train two SVM classifiers with Linear and RBF kernels on the Wine dataset, then compare their accuracies.\n",
        "\n",
        "This program demonstrates how to train two Support Vector Machine (SVM) classifiers: one using a `linear` kernel and another using a `Radial Basis Function (RBF)` kernel. We will use the Wine dataset, split it into training and testing sets, train both models, and then compare their classification accuracies on the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fac919bb",
        "outputId": "e8d4a629-0653-4192-9349-a7146428fe9b"
      },
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# 1. Load the Wine dataset\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "feature_names = wine.feature_names\n",
        "\n",
        "print(f\"Dataset features: {feature_names}\")\n",
        "print(f\"Shape of data (X): {X.shape}\")\n",
        "print(f\"Shape of target (y): {y.shape}\\n\")\n",
        "\n",
        "# 2. Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# 3. Standardize the features (important for SVMs, especially with RBF kernel)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# 4. Initialize and train SVM with Linear Kernel\n",
        "print(\"Training SVM with Linear Kernel...\")\n",
        "svm_linear = SVC(kernel='linear', random_state=42)\n",
        "svm_linear.fit(X_train_scaled, y_train)\n",
        "y_pred_linear = svm_linear.predict(X_test_scaled)\n",
        "accuracy_linear = accuracy_score(y_test, y_pred_linear)\n",
        "print(f\"Accuracy of SVM (Linear Kernel): {accuracy_linear:.4f}\\n\")\n",
        "\n",
        "# 5. Initialize and train SVM with RBF Kernel\n",
        "print(\"Training SVM with RBF Kernel...\")\n",
        "svm_rbf = SVC(kernel='rbf', random_state=42)\n",
        "svm_rbf.fit(X_train_scaled, y_train)\n",
        "y_pred_rbf = svm_rbf.predict(X_test_scaled)\n",
        "accuracy_rbf = accuracy_score(y_test, y_pred_rbf)\n",
        "print(f\"Accuracy of SVM (RBF Kernel): {accuracy_rbf:.4f}\\n\")\n",
        "\n",
        "# 6. Compare accuracies\n",
        "print(\"--- Comparison ---\")\n",
        "if accuracy_linear > accuracy_rbf:\n",
        "    print(f\"The Linear Kernel SVM performed better with an accuracy of {accuracy_linear:.4f}.\")\n",
        "elif accuracy_rbf > accuracy_linear:\n",
        "    print(f\"The RBF Kernel SVM performed better with an accuracy of {accuracy_rbf:.4f}.\")\n",
        "else:\n",
        "    print(f\"Both Linear and RBF Kernel SVMs performed equally with an accuracy of {accuracy_linear:.4f}.\")\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset features: ['alcohol', 'malic_acid', 'ash', 'alcalinity_of_ash', 'magnesium', 'total_phenols', 'flavanoids', 'nonflavanoid_phenols', 'proanthocyanins', 'color_intensity', 'hue', 'od280/od315_of_diluted_wines', 'proline']\n",
            "Shape of data (X): (178, 13)\n",
            "Shape of target (y): (178,)\n",
            "\n",
            "Training SVM with Linear Kernel...\n",
            "Accuracy of SVM (Linear Kernel): 0.9815\n",
            "\n",
            "Training SVM with RBF Kernel...\n",
            "Accuracy of SVM (RBF Kernel): 0.9815\n",
            "\n",
            "--- Comparison ---\n",
            "Both Linear and RBF Kernel SVMs performed equally with an accuracy of 0.9815.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8: What is the Naïve Bayes classifier, and why is it called \"Naïve\"?\n",
        "\n",
        "The **Naïve Bayes classifier** is a family of simple probabilistic classifiers based on applying Bayes' theorem with strong (naïve) independence assumptions between the features. It is a supervised learning algorithm used for classification tasks, particularly popular in text classification, spam filtering, and recommendation systems due to its simplicity and efficiency.\n",
        "\n",
        "What is it?\n",
        "\n",
        "At its core, the Naïve Bayes classifier works by calculating the probability of a data point belonging to a certain class, given the values of its features. It does this by leveraging **Bayes' Theorem**, which states:\n",
        "\n",
        "$P(C|X) = \\frac{P(X|C)P(C)}{P(X)}$\n",
        "\n",
        "Where:\n",
        "*   $P(C|X)$ is the posterior probability: the probability of class $C$ given feature vector $X$.\n",
        "*   $P(X|C)$ is the likelihood: the probability of observing feature vector $X$ given class $C$.\n",
        "*   $P(C)$ is the prior probability: the probability of class $C$ occurring independently.\n",
        "*   $P(X)$ is the evidence: the probability of observing feature vector $X$ independently.\n",
        "\n",
        "For a feature vector $X = (x_1, x_2, ..., x_n)$, Bayes' theorem becomes:\n",
        "\n",
        "$P(C|x_1, ..., x_n) = \\frac{P(x_1, ..., x_n|C)P(C)}{P(x_1, ..., x_n)}$\n",
        "\n",
        "The classifier then predicts the class with the highest posterior probability:\n",
        "\n",
        "$C_{predict} = \\text{argmax}_{C} \\ P(C|X)$\n",
        "\n",
        "**Why is it called \"Naïve\"?**\n",
        "\n",
        "The \"Naïve\" part of the name comes from the fundamental simplification the algorithm makes: **it assumes that all features are independent of each other given the class.**\n",
        "\n",
        "Mathematically, this means:\n",
        "\n",
        "$P(x_1, ..., x_n|C) = P(x_1|C)P(x_2|C)...P(x_n|C)$\n",
        "\n",
        "This assumption significantly simplifies the calculation of the likelihood term $P(X|C)$, making the model computationally efficient. Instead of having to calculate the joint probability of all features given the class, which would require an enormous amount of data and complex calculations, it can simply multiply the individual probabilities of each feature given the class.\n",
        "\n",
        " Implications of the \"Naïve\" Assumption:\n",
        "\n",
        "*   **Simplicity and Speed:** The independence assumption drastically reduces the computational complexity, making Naïve Bayes very fast to train and predict, even with large datasets and many features.\n",
        "*   **Robustness to Irrelevant Features:** If a feature is irrelevant, it theoretically doesn't affect the other features' probabilities given the class, so it shouldn't negatively impact the classification much.\n",
        "*   **\"Zero-Frequency Problem\":** If a category for a feature (in the test data) was not observed in the training data, the likelihood for that feature will be zero, causing the entire posterior probability to be zero. Techniques like Laplace smoothing are used to address this.\n",
        "*   **Performance:** Despite its overly simplistic assumption, Naïve Bayes often performs surprisingly well in practice, especially for tasks where the independence assumption holds reasonably true or when the dataset is not extremely complex. For instance, in spam detection, the presence of certain words (features) often indicates spam regardless of other words, making the independence assumption somewhat acceptable.\n",
        "*   **Interpretability:** The model is relatively easy to understand, as the probabilities provide insights into the importance of different features for each class.\n",
        "\n"
      ],
      "metadata": {
        "id": "-OWZbfzrQXNi"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ac9cc7a3"
      },
      "source": [
        "Question 9: Explain the differences between Gaussian Naïve Bayes, Multinomial Naïve Bayes, and Bernoulli Naïve Bayes\n",
        "\n",
        "While all Naïve Bayes classifiers are based on the core principle of Bayes' Theorem with the assumption of feature independence, they differ primarily in the **assumptions they make about the distribution of the features** given the class. This leads to different ways of calculating the likelihood $P(x_i|C)$.\n",
        "\n",
        "1. Gaussian Naïve Bayes\n",
        "\n",
        "*   **Feature Type:** Assumes that continuous features associated with each class are distributed according to a **Gaussian (normal) distribution**.\n",
        "*   **Likelihood Calculation:** It calculates the probability of a feature value by evaluating its probability density function (PDF) based on the mean and standard deviation of that feature for each class.\n",
        "    *   $P(x_i | C) = \\frac{1}{\\sqrt{2\\pi\\sigma_{C,i}^2}} e^{ -\\frac{(x_i - \\mu_{C,i})^2}{2\\sigma_{C,i}^2} }$\n",
        "    where $\\mu_{C,i}$ is the mean of feature $i$ given class $C$, and $\\sigma_{C,i}^2$ is the variance of feature $i$ given class $C$.\n",
        "*   **Use Cases:** Best suited for datasets where continuous numerical features are expected to follow a normal distribution. For example, classifying a customer as 'high-value' or 'low-value' based on their annual income, where income might be normally distributed within each group.\n",
        "*   **Strengths:** Simple, fast, and works well for continuous data that is Gaussian or can be approximated as such.\n",
        "*   **Weaknesses:** Assumes Gaussian distribution, which might not hold for all continuous data. Can be sensitive to outliers if not handled.\n",
        "\n",
        "2. Multinomial Naïve Bayes\n",
        "\n",
        "*   **Feature Type:** Assumes that features represent **counts or frequencies** (e.g., word counts in a document). It works with discrete data and is particularly well-suited for text classification problems where features are term frequencies or presence counts.\n",
        "*   **Likelihood Calculation:** It assumes a multinomial distribution for the features. The likelihood $P(x_i|C)$ is calculated based on the proportion of times feature $x_i$ appears in class $C$ documents relative to the total number of features in class $C$.\n",
        "    *   $P(x_i | C) = \\frac{N_{C,i} + \\alpha}{N_C + \\alpha n}$\n",
        "    where $N_{C,i}$ is the count of feature $i$ in class $C$ examples, $N_C$ is the total count of all features for class $C$, $n$ is the number of features, and $\\alpha$ is a smoothing parameter (Laplace smoothing).\n",
        "*   **Use Cases:** Widely used in Natural Language Processing (NLP) tasks such as spam detection, document classification, sentiment analysis (where features are typically word counts or TF-IDF values).\n",
        "*   **Strengths:** Excellent for text classification, handles sparse data well, and is very fast.\n",
        "*   **Weaknesses:** Not suitable for features that are not counts or frequencies. Performs poorly if features are binary or negative.\n",
        "\n",
        "3. Bernoulli Naïve Bayes\n",
        "\n",
        "*   **Feature Type:** Assumes that features are **binary (Boolean) variables**, meaning they can only take two values (e.g., presence or absence of a word). It is similar to Multinomial Naïve Bayes but explicitly models the non-occurrence of features.\n",
        "*   **Likelihood Calculation:** It assumes a Bernoulli distribution. The likelihood $P(x_i|C)$ for a feature $x_i$ is calculated as the probability of its presence (1) or absence (0) given the class $C$.\n",
        "    *   $P(x_i=1 | C) = P_{C,i}$ (probability of feature $i$ appearing in class $C$)\n",
        "    *   $P(x_i=0 | C) = 1 - P_{C,i}$ (probability of feature $i$ not appearing in class $C$)\n",
        "*   **Use Cases:** Also used in text classification, especially when dealing with very short texts or when only the presence/absence of words matters, rather than their frequency. For example, spam classification where a word either exists or not.\n",
        "*   **Strengths:** Good for binary feature data, simple to implement, and handles non-occurrence of features explicitly.\n",
        "*   **Weaknesses:** Only works for binary features. May lose information if feature frequencies are important.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62361842"
      },
      "source": [
        "Question 10: Breast Cancer Dataset\n",
        "\n",
        "Write a Python program to train a Gaussian Naïve Bayes classifier on the Breast Cancer dataset and evaluate accuracy.\n",
        "\n",
        "This program will demonstrate how to train a Gaussian Naïve Bayes classifier. We will use the Breast Cancer dataset from `sklearn.datasets`, split it into training and testing sets, train the model, and then evaluate its accuracy on the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cf32a710",
        "outputId": "a2687e41-4ea4-4011-a208-7208b909ea52"
      },
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load the Breast Cancer dataset\n",
        "breast_cancer = load_breast_cancer()\n",
        "X = breast_cancer.data\n",
        "y = breast_cancer.target\n",
        "feature_names = breast_cancer.feature_names\n",
        "\n",
        "print(f\"Dataset features: {list(feature_names)}\")\n",
        "print(f\"Shape of data (X): {X.shape}\")\n",
        "print(f\"Shape of target (y): {y.shape}\\n\")\n",
        "\n",
        "# 2. Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# 3. Initialize the Gaussian Naïve Bayes classifier\n",
        "gnb = GaussianNB()\n",
        "\n",
        "# 4. Train the classifier on the training data\n",
        "print(\"Training Gaussian Naïve Bayes classifier...\")\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# 5. Make predictions on the test data\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "# 6. Evaluate the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"\\nGaussian Naïve Bayes Classifier Accuracy: {accuracy:.4f}\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset features: [np.str_('mean radius'), np.str_('mean texture'), np.str_('mean perimeter'), np.str_('mean area'), np.str_('mean smoothness'), np.str_('mean compactness'), np.str_('mean concavity'), np.str_('mean concave points'), np.str_('mean symmetry'), np.str_('mean fractal dimension'), np.str_('radius error'), np.str_('texture error'), np.str_('perimeter error'), np.str_('area error'), np.str_('smoothness error'), np.str_('compactness error'), np.str_('concavity error'), np.str_('concave points error'), np.str_('symmetry error'), np.str_('fractal dimension error'), np.str_('worst radius'), np.str_('worst texture'), np.str_('worst perimeter'), np.str_('worst area'), np.str_('worst smoothness'), np.str_('worst compactness'), np.str_('worst concavity'), np.str_('worst concave points'), np.str_('worst symmetry'), np.str_('worst fractal dimension')]\n",
            "Shape of data (X): (569, 30)\n",
            "Shape of target (y): (569,)\n",
            "\n",
            "Training Gaussian Naïve Bayes classifier...\n",
            "\n",
            "Gaussian Naïve Bayes Classifier Accuracy: 0.9415\n"
          ]
        }
      ]
    }
  ]
}